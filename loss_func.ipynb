{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8eef297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59.6450, grad_fn=<SubBackward0>)\n",
      "tensor(-59.6450, grad_fn=<SubBackward0>)\n",
      "tensor(67.9840, grad_fn=<SubBackward0>)\n",
      "tensor(-67.9840, grad_fn=<SubBackward0>)\n",
      "tensor(22.3741, grad_fn=<SubBackward0>)\n",
      "tensor(-22.3741, grad_fn=<SubBackward0>)\n",
      "tensor(-5.7841, grad_fn=<SubBackward0>)\n",
      "tensor(5.7841, grad_fn=<SubBackward0>)\n",
      "tensor(2.2667, grad_fn=<SubBackward0>)\n",
      "tensor(-2.2667, grad_fn=<SubBackward0>)\n",
      "tensor(9.5218, grad_fn=<SubBackward0>)\n",
      "tensor(-9.5218, grad_fn=<SubBackward0>)\n",
      "tensor(116.7105, grad_fn=<SubBackward0>)\n",
      "tensor(-116.7105, grad_fn=<SubBackward0>)\n",
      "tensor(2.9252, grad_fn=<SubBackward0>)\n",
      "tensor(-2.9252, grad_fn=<SubBackward0>)\n",
      "tensor(91.3079, grad_fn=<SubBackward0>)\n",
      "tensor(-91.3079, grad_fn=<SubBackward0>)\n",
      "tensor(42.3505, grad_fn=<SubBackward0>)\n",
      "tensor(-42.3505, grad_fn=<SubBackward0>)\n",
      "tensor(12.5792, grad_fn=<SubBackward0>)\n",
      "tensor(-12.5792, grad_fn=<SubBackward0>)\n",
      "tensor(10.1391, grad_fn=<SubBackward0>)\n",
      "tensor(-10.1391, grad_fn=<SubBackward0>)\n",
      "tensor(68.9788, grad_fn=<SubBackward0>)\n",
      "tensor(-68.9788, grad_fn=<SubBackward0>)\n",
      "tensor(5.9205, grad_fn=<SubBackward0>)\n",
      "tensor(-5.9205, grad_fn=<SubBackward0>)\n",
      "tensor(95.7143, grad_fn=<SubBackward0>)\n",
      "tensor(-95.7143, grad_fn=<SubBackward0>)\n",
      "tensor(24.9407, grad_fn=<SubBackward0>)\n",
      "tensor(-24.9407, grad_fn=<SubBackward0>)\n",
      "tensor(43.7685, grad_fn=<SubBackward0>)\n",
      "tensor(-43.7685, grad_fn=<SubBackward0>)\n",
      "tensor(54.2387, grad_fn=<SubBackward0>)\n",
      "tensor(-54.2387, grad_fn=<SubBackward0>)\n",
      "tensor(83.7937, grad_fn=<SubBackward0>)\n",
      "tensor(-83.7937, grad_fn=<SubBackward0>)\n",
      "tensor(80.6115, grad_fn=<SubBackward0>)\n",
      "tensor(-80.6115, grad_fn=<SubBackward0>)\n",
      "tensor(77.2390, grad_fn=<SubBackward0>)\n",
      "tensor(-77.2390, grad_fn=<SubBackward0>)\n",
      "tensor(28.8275, grad_fn=<SubBackward0>)\n",
      "tensor(-28.8275, grad_fn=<SubBackward0>)\n",
      "tensor(111.8872, grad_fn=<SubBackward0>)\n",
      "tensor(-111.8872, grad_fn=<SubBackward0>)\n",
      "tensor(66.8985, grad_fn=<SubBackward0>)\n",
      "tensor(-66.8985, grad_fn=<SubBackward0>)\n",
      "tensor(55.9629, grad_fn=<SubBackward0>)\n",
      "tensor(-55.9629, grad_fn=<SubBackward0>)\n",
      "tensor(12.7818, grad_fn=<SubBackward0>)\n",
      "tensor(-12.7818, grad_fn=<SubBackward0>)\n",
      "tensor(39.1273, grad_fn=<SubBackward0>)\n",
      "tensor(-39.1273, grad_fn=<SubBackward0>)\n",
      "tensor(9.9083, grad_fn=<SubBackward0>)\n",
      "tensor(-9.9083, grad_fn=<SubBackward0>)\n",
      "tensor(24.0854, grad_fn=<SubBackward0>)\n",
      "tensor(-24.0854, grad_fn=<SubBackward0>)\n",
      "tensor(-21.3792, grad_fn=<SubBackward0>)\n",
      "tensor(21.3792, grad_fn=<SubBackward0>)\n",
      "tensor(90.3608, grad_fn=<SubBackward0>)\n",
      "tensor(-90.3608, grad_fn=<SubBackward0>)\n",
      "tensor(28.4980, grad_fn=<SubBackward0>)\n",
      "tensor(-28.4980, grad_fn=<SubBackward0>)\n",
      "tensor(70.9333, grad_fn=<SubBackward0>)\n",
      "tensor(-70.9333, grad_fn=<SubBackward0>)\n",
      "tensor(49.3004, grad_fn=<SubBackward0>)\n",
      "tensor(-49.3004, grad_fn=<SubBackward0>)\n",
      "tensor(78.9937, grad_fn=<SubBackward0>)\n",
      "tensor(-78.9937, grad_fn=<SubBackward0>)\n",
      "tensor(-14.3868, grad_fn=<SubBackward0>)\n",
      "tensor(14.3868, grad_fn=<SubBackward0>)\n",
      "tensor(32.9614, grad_fn=<SubBackward0>)\n",
      "tensor(-32.9614, grad_fn=<SubBackward0>)\n",
      "tensor(-0.6851, grad_fn=<SubBackward0>)\n",
      "tensor(0.6851, grad_fn=<SubBackward0>)\n",
      "tensor(116.2854, grad_fn=<SubBackward0>)\n",
      "tensor(-116.2854, grad_fn=<SubBackward0>)\n",
      "tensor(61.8987, grad_fn=<SubBackward0>)\n",
      "tensor(-61.8987, grad_fn=<SubBackward0>)\n",
      "tensor(-9.3854, grad_fn=<SubBackward0>)\n",
      "tensor(9.3854, grad_fn=<SubBackward0>)\n",
      "tensor(-2.1422, grad_fn=<SubBackward0>)\n",
      "tensor(2.1422, grad_fn=<SubBackward0>)\n",
      "tensor(11.7705, grad_fn=<SubBackward0>)\n",
      "tensor(-11.7705, grad_fn=<SubBackward0>)\n",
      "tensor(18.0916, grad_fn=<SubBackward0>)\n",
      "tensor(-18.0916, grad_fn=<SubBackward0>)\n",
      "tensor(28.2821, grad_fn=<SubBackward0>)\n",
      "tensor(-28.2821, grad_fn=<SubBackward0>)\n",
      "tensor(-1.6945, grad_fn=<SubBackward0>)\n",
      "tensor(1.6945, grad_fn=<SubBackward0>)\n",
      "tensor(52.8349, grad_fn=<SubBackward0>)\n",
      "tensor(-52.8349, grad_fn=<SubBackward0>)\n",
      "tensor(68.1665, grad_fn=<SubBackward0>)\n",
      "tensor(-68.1665, grad_fn=<SubBackward0>)\n",
      "tensor(62.0590, grad_fn=<SubBackward0>)\n",
      "tensor(-62.0590, grad_fn=<SubBackward0>)\n",
      "tensor(47.9384, grad_fn=<SubBackward0>)\n",
      "tensor(-47.9384, grad_fn=<SubBackward0>)\n",
      "tensor(25.8181, grad_fn=<SubBackward0>)\n",
      "tensor(-25.8181, grad_fn=<SubBackward0>)\n",
      "tensor(54.8327, grad_fn=<SubBackward0>)\n",
      "tensor(-54.8327, grad_fn=<SubBackward0>)\n",
      "tensor(52.7107, grad_fn=<SubBackward0>)\n",
      "tensor(-52.7107, grad_fn=<SubBackward0>)\n",
      "tensor(94.0531, grad_fn=<SubBackward0>)\n",
      "tensor(-94.0531, grad_fn=<SubBackward0>)\n",
      "tensor(83.3949, grad_fn=<SubBackward0>)\n",
      "tensor(-83.3949, grad_fn=<SubBackward0>)\n",
      "tensor(47.6335, grad_fn=<SubBackward0>)\n",
      "tensor(-47.6335, grad_fn=<SubBackward0>)\n",
      "tensor(26.2250, grad_fn=<SubBackward0>)\n",
      "tensor(-26.2250, grad_fn=<SubBackward0>)\n",
      "tensor(97.4631, grad_fn=<SubBackward0>)\n",
      "tensor(-97.4631, grad_fn=<SubBackward0>)\n",
      "tensor(94.1271, grad_fn=<SubBackward0>)\n",
      "tensor(-94.1271, grad_fn=<SubBackward0>)\n",
      "tensor(94.0043, grad_fn=<SubBackward0>)\n",
      "tensor(-94.0043, grad_fn=<SubBackward0>)\n",
      "tensor(5.8466, grad_fn=<SubBackward0>)\n",
      "tensor(-5.8466, grad_fn=<SubBackward0>)\n",
      "tensor(20.9204, grad_fn=<SubBackward0>)\n",
      "tensor(-20.9204, grad_fn=<SubBackward0>)\n",
      "tensor(-1.4426, grad_fn=<SubBackward0>)\n",
      "tensor(1.4426, grad_fn=<SubBackward0>)\n",
      "tensor(61.6825, grad_fn=<SubBackward0>)\n",
      "tensor(-61.6825, grad_fn=<SubBackward0>)\n",
      "tensor(68.3152, grad_fn=<SubBackward0>)\n",
      "tensor(-68.3152, grad_fn=<SubBackward0>)\n",
      "tensor(22.7194, grad_fn=<SubBackward0>)\n",
      "tensor(-22.7194, grad_fn=<SubBackward0>)\n",
      "tensor(78.2905, grad_fn=<SubBackward0>)\n",
      "tensor(-78.2905, grad_fn=<SubBackward0>)\n",
      "tensor(79.9491, grad_fn=<SubBackward0>)\n",
      "tensor(-79.9491, grad_fn=<SubBackward0>)\n",
      "tensor(5.7141, grad_fn=<SubBackward0>)\n",
      "tensor(-5.7141, grad_fn=<SubBackward0>)\n",
      "tensor(78.5376, grad_fn=<SubBackward0>)\n",
      "tensor(-78.5376, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "for x in range(70):\n",
    "    posweight = torch.randint(1, 100, (1,)).float()\n",
    "    \n",
    "    critrion = nn.BCEWithLogitsLoss(pos_weight=posweight)\n",
    "    a = torch.randn(15, 3, requires_grad=True)\n",
    "    b = torch.randint(0, 4, (15, 3)).float()\n",
    "    \n",
    "    loss = critrion(a, b)\n",
    "    \n",
    "    critrionrw = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    lossrw = critrionrw(a, b)\n",
    "    weight = torch.ones_like(lossrw)\n",
    "    weight[b==1.] = poswight\n",
    "    lossrw = (lossrw * weight).mean()\n",
    "    \n",
    "    critrionrw_sig = nn.BCELoss(reduction='none')\n",
    "    lossrw_sig = critrionrw_sig(torch.sigmoid(a), b)\n",
    "    lossrw_sig = ((lossrw_sig) * weight).mean()\n",
    "\n",
    "    print(loss - lossrw)    \n",
    "    print(lossrw_sig - loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5505a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.BCELoss(reduction='none', weight = torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0cefa04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0833dc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12396/2348593626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcritrionrw_sig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'weight'"
     ]
    }
   ],
   "source": [
    "critrionrw_sig(torch.sigmoid(a), b, weight = torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da957ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26.0511, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossrw_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26940c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [78.,  1.,  1.],\n",
       "        [ 1., 78.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [78., 78.,  1.],\n",
       "        [ 1.,  1., 78.],\n",
       "        [78.,  1., 78.],\n",
       "        [78.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1., 78.,  1.],\n",
       "        [78.,  1.,  1.],\n",
       "        [ 1., 78., 78.],\n",
       "        [ 1.,  1., 78.],\n",
       "        [ 1., 78.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935b6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
